{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lending Club releases data for all of the approved and declined loan applications periodically on their website. You can select a few different year ranges to download the datasets (in CSV format) for both approved and declined loans.\n",
    "\n",
    "You'll also find a data dictionary (in XLS format) which contains information on the different column names towards the bottom of the page. This data dictionary is available in the data/financial loans repo, so you can refer to it whenever anyone would to learn more about what a column represents in the datasets.\n",
    "\n",
    "Can we build a machine learning model that can accurately predict if a borrower will pay off their loan on time or not?\n",
    "\n",
    "In this notebook, we'll build a machine learning model that will focus on approved loans data from 2007 to 2011, since a good number of the loans have already finished. In the datasets for later years, many of the loans are current and still being paid off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that code runs fast on our platform, we reduced the size of LoanStats3a.csv by:\n",
    "\n",
    "(1) Removing the first line because it contains the extraneous text Notes offered by Prospectus (https://www.lendingclub.com/info/prospectus.action) instead of the column titles, which prevents the dataset from being parsed by the pandas library properly.\n",
    "\n",
    "(2) Removing the desc column which contains a long text explanation for each loan.\n",
    "\n",
    "(3) Removing the url column which contains a link to each loan on Lending Club which can only be accessed with an investor account.\n",
    "\n",
    "(4) Removing all columns containing more than 50% missing values which allows us to move faster since we can spend less time trying to fill these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir(r\"C:\\Users\\gerr1\\Desktop\\Data Science Portfolio\\data\\financial loans\")\n",
    "\n",
    "#The chunck of code below performs the data cleaning described above\n",
    "loans_2007 = pd.read_csv('LoanStats3a.csv', skiprows=1)\n",
    "half_count = len(loans_2007) / 2\n",
    "loans_2007 = loans_2007.dropna(thresh=half_count, axis=1)\n",
    "loans_2007 = loans_2007.drop(['desc', 'url'],axis=1)\n",
    "loans_2007.to_csv('loans_2007.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "loans_2007 = pd.read_csv(\"loans_2007.csv\", sep=',', error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "loans_2007.drop_duplicates\n",
    "\n",
    "print(loans_2007.shape)\n",
    "loans_2007.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Printing column dtypes\n",
    "print(loans_2007.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, pandas had a little trouble figuring out what dtype each column is. This means we may have to manually assign some columns to float64 and convert some of the other string columns to numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe contains many columns and can be cumbersome to try to explore all at once. We will break up the columns into 3 groups of 18 columns and use the data dictionary to become familiar with what each column represents. As we understand each feature, we will pay attention to any features that leak information from the future (after the loan has already been funded), don't affect a borrower's ability to pay back a loan (e.g. a randomly generated ID value by Lending Club), formatted poorly and need to be cleaned up, and/or require more data or a lot of processing to turn into a useful feature, contain redundant information.\n",
    "\n",
    "In the next few cells, we'll focus on just columns that we need to remove from consideration. Then, we can circle back and further dissect the columns we decided to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing each column, we can conclude that the following features need to be removed:\n",
    "\n",
    "id: randomly generated field by Lending Club for unique identification purposes only,\n",
    "member_id: also a randomly generated field by Lending Club for unique identification purposes only,\n",
    "funded_amnt: leaks data from the future (after the loan is already started to be funded),\n",
    "funded_amnt_inv: also leaks data from the future (after the loan is already started to be funded),\n",
    "grade: contains redundant information as the interest rate column (int_rate),\n",
    "sub_grade: also contains redundant information as the interest rate column (int_rate),\n",
    "emp_title: requires other data and a lot of processing to potentially be useful, and\n",
    "issue_d: leaks data from the future (after the loan is already completed funded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dropping redundant columns\n",
    "drop_cols = [\"id\", \"member_id\", \"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"sub_grade\", \"emp_title\", \"issue_d\"]\n",
    "\n",
    "loans = loans_2007.drop(drop_cols, axis=1)\n",
    "print(loans.shape)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to drop the following columns:\n",
    "\n",
    "zip_code: redundant with the addr_state column since only the first 3 digits of the 5 digit zip code are visible (which only can be used to identify the state the borrower lives in), \n",
    "out_prncp: leaks data from the future, (after the loan already started to be paid off), \n",
    "out_prncp_inv: also leaks data from the future, (after the loan already started to be paid off), \n",
    "total_pymnt: also leaks data from the future, (after the loan already started to be paid off), \n",
    "total_pymnt_inv: also leaks data from the future, (after the loan already started to be paid off), \n",
    "total_rec_prncp: also leaks data from the future, (after the loan already started to be paid off).\n",
    "\n",
    "The out_prncp and out_prncp_inv both describe the outstanding principal amount for a loan, which is the remaining amount the borrower still owes. These 2 columns as well as the total_pymnt column describe properties of the loan after it's fully funded and started to be paid off. This information isn't available to an investor before the loan is fully funded and we don't want to include it in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = loans.drop([\"zip_code\", \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loans.shape)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we need to drop the following columns:\n",
    "\n",
    "total_rec_int: leaks data from the future, (after the loan already started to be paid off),\n",
    "total_rec_late_fee: also leaks data from the future, (after the loan already started to be paid off),\n",
    "recoveries: also leaks data from the future, (after the loan already started to be paid off),\n",
    "collection_recovery_fee: also leaks data from the future, (after the loan already started to be paid off),\n",
    "last_pymnt_d: also leaks data from the future, (after the loan already started to be paid off),\n",
    "last_pymnt_amnt: also leaks data from the future, (after the loan already started to be paid off).\n",
    "\n",
    "All of these columns leak data from the future, meaning that they're describing aspects of the loan after it's already been fully funded and started to be paid off by the borrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = loans.drop([\"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shape and first row\n",
    "print(loans.shape)\n",
    "loans.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Column & Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column we will be using as a target is the loan_status column since it indicates whether the loan was fully paid. But, this column has categorical values, which we will need to get dummy values for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, we'll take a look at the loan_status column\n",
    "print(loans_2007[\"loan_status\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 different possible values for the loan_status column.\n",
    "\n",
    "From the investor's perspective, we're interested in trying to predict which loans will be paid off on time and which ones won't be. Only the Fully Paid and Charged Off values describe the final outcome of the loan. The other values describe loans that are still on going and where the jury is still out on if the borrower will pay back the loan on time or not. While the Default status resembles the Charged Off status, in Lending Club's eyes, loans that are charged off have essentially no chance of being repaid while default ones have a small chance.\n",
    "\n",
    "Since we're interesting in being able to predict which of these 2 values a loan will fall under, we can treat the problem as a binary classification one. Let's remove all the loans that don't contain either Fully Paid and Charged Off as the loan's status and then transform the Fully Paid values to 1 for the positive case and the Charged Off values to 0 for the negative case. While there are a few different ways to transform all of the values in a column, we'll use the Dataframe method replace. According to the documentation, we can pass the replace method a nested mapping dictionary in the following format:\n",
    "\n",
    "    mapping_dict = {\n",
    "        \"date\": {\n",
    "            \"january\": 1,\n",
    "            \"february\": 2,\n",
    "            \"march\": 3\n",
    "        }\n",
    "    }\n",
    "    df = df.replace(mapping_dict)\n",
    "\n",
    "Lastly, one thing we need to keep in mind is the class imbalance between the positive and negative cases. While there are 33,136 loans that have been fully paid off, there are only 5,634 that were charged off. This class imbalance is a common problem in binary classification and during training, the model ends up having a strong bias towards predicting the class with more observations in the training set and will rarely predict the class with less observations. The stronger the imbalance, the more biased the model becomes. There are a few different ways to tackle this class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removing all rows from loans_2007 that contain values other than Fully Paid or Charged Off for the loan_status column\n",
    "loans = loans[(loans[\"loan_status\"] == \"Fully Paid\") | (loans[\"loan_status\"] == \"Charged Off\")]\n",
    "\n",
    "#Using the Dataframe method replace to replace: Fully Paid with 1, Charged Off with 0\n",
    "replacements = {\"Fully Paid\": 1, \"Charged Off\": 0}\n",
    "loans[\"loan_status\"] = loans[\"loan_status\"].replace(to_replace=replacements.keys(), value=replacements.values())\n",
    "\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Single Value Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look for any columns that contain only one unique value and remove them. These columns won't be useful for the model since they don't add any information to each loan application. In addition, removing these columns will reduce the number of columns we'll need to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removing any columns from loans_2007 that contain only one unique value\n",
    "drop_columns = []\n",
    "\n",
    "for col in list(loans.columns):\n",
    "    non_null = loans[col].dropna()\n",
    "    unique_non_null = non_null.unique()\n",
    "    num_true_unique = len(unique_non_null)\n",
    "    if num_true_unique <= 1:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "loans = loans.drop(drop_columns, axis=1)\n",
    "\n",
    "print(drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shape of filtered data\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks we we were able to remove 9 more columns since they only contained 1 unique value. Now, we'll explore the individual features in greater depth and work towards training our first machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now prepare the data for machine learning by focusing on handling missing values, converting categorical columns to numeric columns, and removing any other extraneous columns we encounter throughout this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use the isnull and sum methods to return the number of null values in each column\n",
    "null_count = loans.isnull().sum()\n",
    "print(null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the columns have 0 missing values, 2 columns have 50 or less rows with missing values, and 1 column, pub_rec_bankruptcies, contains 697 rows with missing values. Let's remove columns entirely where more than 1% of the rows for that column contain a null value. In addition, we'll remove the remaining rows containing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans = loans.drop(\"pub_rec_bankruptcies\", axis=1)\n",
    "loans = loans.dropna(axis=0)\n",
    "print(loans.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use the Dataframe method select_dtypes to select only the columns of object type from loans and assign the resulting \n",
    "#Dataframe object_columns_df\n",
    "object_columns_df = loans.select_dtypes(include=[\"object\"])\n",
    "print(object_columns_df.shape)\n",
    "object_columns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns seem like they represent categorical values, but we should confirm by checking the number of unique values in those columns:\n",
    "\n",
    "    home_ownership: home ownership status, can only be 1 of 4 categorical values according to the data dictionary,\n",
    "    verification_status: indicates if income was verified by Lending Club,\n",
    "    emp_length: number of years the borrower was employed upon time of application,\n",
    "    term: number of payments on the loan, either 36 or 60,\n",
    "    addr_state: borrower's state of residence,\n",
    "    purpose: a category provided by the borrower for the loan request,\n",
    "    title: loan title provided the borrower.\n",
    "\n",
    "We're also left with some columns that need to be converted to numeric like loan_amnt, revol_util and int_rate.\n",
    "\n",
    "Lastly, some of the columns contain date values that would require a good amount of feature engineering for them to be potentially useful:\n",
    "\n",
    "    earliest_cr_line: The month the borrower's earliest reported credit line was opened,\n",
    "    last_credit_pull_d: The most recent month Lending Club pulled credit for this loan.\n",
    "\n",
    "Since these date features require some feature engineering for modeling purposes, let's remove these date columns from the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's explore the unique value counts of the columnns that seem like they contain categorical values\n",
    "cat_cols = ['home_ownership', 'verification_status', 'emp_length', 'term', 'addr_state']\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(loans[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The home_ownership, verification_status, emp_length, term, and addr_state columns all contain multiple discrete values. We should clean the emp_length column and treat it as a numerical one since the values have ordering (2 years of employment is less than 8 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's look at the unique value counts for the purpose and title columns to understand which column we want to keep\n",
    "print(loans[\"purpose\"].value_counts())\n",
    "print(loans[\"title\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The home_ownership, verification_status, emp_length, and term columns each contain a few discrete categorical values. We should encode these columns as dummy variables and keep them.\n",
    "\n",
    "It seems like the purpose and title columns do contain overlapping information but we'll keep the purpose column since it contains a few discrete values. In addition, the title column has data quality issues since many of the values are repeated with slight modifications (e.g. Debt Consolidation and Debt Consolidation Loan and debt consolidation).\n",
    "\n",
    "We can use the following mapping to clean the emp_length column:\n",
    "\n",
    "    \"10+ years\": 10\n",
    "    \"9 years\": 9\n",
    "    \"8 years\": 8\n",
    "    \"7 years\": 7\n",
    "    \"6 years\": 6\n",
    "    \"5 years\": 5\n",
    "    \"4 years\": 4\n",
    "    \"3 years\": 3\n",
    "    \"2 years\": 2\n",
    "    \"1 year\": 1\n",
    "    \"< 1 year\": 0\n",
    "    \"n/a\": 0\n",
    "    \n",
    "We erred on the side of being conservative with the 10+ years, < 1 year and n/a mappings. We assume that people who may have been working more than 10 years have only really worked for 10 years. We also assume that people who've worked less than a year or if the information is not available that they've worked for 0. This is a general heuristic but it's not perfect.\n",
    "\n",
    "Lastly, the addr_state column contains many discrete values and we'd need to add 49 dummy variable columns to use it for classification. This would make our Dataframe much larger and could slow down how quickly the code runs. Let's remove this column from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the last_credit_pull_d, addr_state, title, and earliest_cr_line columns from loans\n",
    "rem_cols = [\"last_credit_pull_d\", \"addr_state\", \"title\", \"earliest_cr_line\"]\n",
    "loans = loans.drop(rem_cols, axis=1)\n",
    "\n",
    "#Convert the int_rate and revol_util columns to float columns\n",
    "loans[\"int_rate\"] = loans['int_rate'].str.rstrip('%').astype(float)\n",
    "loans[\"revol_util\"] = loans['revol_util'].str.rstrip('%').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use the replace method to clean the emp_length column\n",
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"7 years\": 7,\n",
    "        \"6 years\": 6,\n",
    "        \"5 years\": 5,\n",
    "        \"4 years\": 4,\n",
    "        \"3 years\": 3,\n",
    "        \"2 years\": 2,\n",
    "        \"1 year\": 1,\n",
    "        \"< 1 year\": 0,\n",
    "        \"n/a\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "loans = loans.replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we just need to convert the remaining numeric columns to the float or int dtype\n",
    "import numpy as np\n",
    "\n",
    "float_cols = ['loan_amnt', 'installment', 'annual_inc', 'dti', 'revol_bal']\n",
    "int_cols = ['delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'total_acc']\n",
    "\n",
    "for col in float_cols:\n",
    "    loans[col] = loans[col].astype(float)\n",
    "    \n",
    "for col in int_cols:\n",
    "    column = loans[col].astype(float)\n",
    "    loans[col] = column.astype(np.int64)\n",
    "    \n",
    "print(loans.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encoding the home_ownership, verification_status, emp_length, purpose, and term columns as integer values\n",
    "cat_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\"]\n",
    "\n",
    "dummy_df = pd.get_dummies(loans[cat_columns])\n",
    "loans = pd.concat([loans, dummy_df], axis=1)\n",
    "loans = loans.drop(cat_columns, axis=1)\n",
    "\n",
    "print(loans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dropping the pymnt_plan column because it has 'n': 39016 and 'y': 1. So this column is not giving any new information\n",
    "loans = loans.drop('pymnt_plan', axis=1)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've performed the last amount of data preparation necessary to start training machine learning models. We converted all of the columns to numerical values because those are the only type of values scikit-learn can work with. In the next section, we'll experiment with training models and evaluating accuracy using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking An Error Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error metric will help us figure out when our model is performing well, and when it's performing poorly. To tie error metrics all the way back to the original question we wanted to answer, let's say we're using a machine learning model to predict whether or not we should fund a loan on the Lending Club platform. Our objective in this is to make money -- we want to fund enough loans that are paid off on time to offset our losses from loans that aren't paid off. An error metric will help us determine if our algorithm will make us money or lose us money.\n",
    "\n",
    "In this case, we're primarily concerned with false positives and false negatives. Both of these are different types of misclassifications. With a false positive, we predict that a loan will be paid off on time, but it actually isn't. This costs us money, since we fund loans that lose us money. With a false negative, we predict that a loan won't be paid off on time, but it actually would be paid off on time. This loses us potential money, since we didn't fund a loan that actually would have been paid off.\n",
    "\n",
    "We are interested in the false positive rate (fpr) and the true positive rate (tpr)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that there is a significant class imbalance in the loan_status column. There are 6 times as many loans that were paid off on time (1), than loans that weren't paid off on time (0). This causes a major issue when we use accuracy as a metric. This is because due to the class imbalance, a classifier can predict 1 for every row, and still have high accuracy. An example follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict that all loans will be paid off on time.\n",
    "predictions = pd.Series(np.ones(loans.shape[0]))\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate:\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last cell, you may have noticed that both fpr and tpr were 1. This is because we predicted 1 for each row. This means that we correctly identified all of the good loans (true positive rate), but we also incorrectly identified all of the bad loans (false positive rate). Now that we've setup error metrics, we can move on to making predictions using a machine learning algorithm. In order to fit the machine learning models, we'll use the Scikit-learn library, in this case, we'll train a logistic regression model.\n",
    "\n",
    "In order to get a realistic depiction of the accuracy of the algorithm, we'll need to use cross validation to generate predictions. Cross validation splits the dataset into groups, then makes predictions on each group using the other groups as training data. This ensures that we don't overfit by generating predictions on the same data that we train our algorithm with.\n",
    "\n",
    "We can perform cross validation using the cross_val_predict method of scikit-learn. cross_val_predict allows us to pass in a classifier, the features, and the target.\n",
    "\n",
    "We'll create an instance of KFold, which will perform 3 fold cross validation across our dataset. We set random_state to 1 to ensure that the folds are always consistent, and we can compare scores between runs. If we don't, each fold will be randomized every time, making it hard to tell if we're improving our model or not.\n",
    "\n",
    "If we pass the instance of KFold into cross_val_predict, it will then perform 3 fold cross validation to generate unbiased predictions.\n",
    "\n",
    "Once we have cross validated predictions, we can compute true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_predict, KFold\n",
    "\n",
    "#Create a Dataframe named features that contains just the feature columns\n",
    "features = loans.drop(\"loan_status\", axis=1)\n",
    "\n",
    "#Create a Series named target that contains just the target column\n",
    "target = loans[\"loan_status\"]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "kf = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "#Generate cross validated predictions for features\n",
    "predictions = cross_val_predict(lr, features, y=target, cv=kf)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (loans[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (loans[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (loans[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (loans[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate:\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the last cell, our fpr and tpr are around what we'd expect if the model was predicting all ones.\n",
    "\n",
    "We will tell the classifier to penalize certain rows more, which is actually much easier to implement using scikit-learn. We can do this by setting the class_weight parameter to balanced when creating the LogisticRegression instance. We can repeat the cross validation procedure we performed in the last cell, but with the class_weight parameter set to balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "kfold = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, y=target, cv=kfold)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (target == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (target == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (target == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (target == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate;\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We significantly improved false positive rate in the last cell by balancing the classes, which reduced true positive rate. Our true positive rate is now around 63%, and our false positive rate is around 61%. From a conservative investor's standpoint, it's reassuring that the false positive rate is lower because it means that we'll be able to do a better job at avoiding bad loans than if we funded everything. However, we'd only ever decide to fund 63% of the total loans (true positive rate), so we'd immediately reject a good amount of loans.\n",
    "\n",
    "We can try to lower the false positive rate further by assigning a harsher penalty for misclassifying the negative class. While setting class_weight to balanced will automatically set a penalty based on the number of 1s and 0s in the column, we can also set a manual penalty. In the last screen, the penalty scikit-learn imposed for misclassifying a 0 would have been around 5.89 (since there are 5.89 times as many 1s as 0s). We will use a dictionary to manually set these weights. We will also try some larger penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will now train a new model with the new manual weights, penalty will be the dict we use\n",
    "penalty = {0: 5.89, 1: 1}\n",
    "\n",
    "#Now we will re-run our new model and test our predictions\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "kfold = KFold(features.shape[0], random_state=2)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, y=target, cv=kfold)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (target == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (target == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (target == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (target == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate;\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can even train a model with some larger manual penalties\n",
    "penalty = {0: 10, 1: 1}\n",
    "\n",
    "#Now we will re-run our new model and test our predictions\n",
    "lr = LogisticRegression(class_weight=penalty)\n",
    "kfold = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(lr, features, y=target, cv=kfold)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (target == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (target == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (target == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (target == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate;\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like assigning manual penalties lowered the false positive rate to 19%, and thus lowered our risk. Note that this comes at the expense of true positive rate. While we have fewer false positives, we're also missing opportunities to fund more loans and potentially make more money. Given that we're approaching this as a conservative investor, this strategy makes sense, but it's worth keeping in mind the tradeoffs.\n",
    "\n",
    "While we could tweak the penalties further, it's best to move to trying a different model right now, for larger potential false positive rate gains. We can always loop back and interate on the penalties more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are able to work with nonlinear data, and learn complex conditionals. Logistic regressions are only able to work with linear data. Training a random forest algorithm may enable us to get more accuracy due to columns that correlate nonlinearly with loan_status.\n",
    "\n",
    "We can use the RandomForestClassifer class from scikit-learn to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Now we will train our Random Forest and analyze the predictions\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=8)\n",
    "kfold = KFold(features.shape[0], random_state=1)\n",
    "\n",
    "predictions = cross_val_predict(rf, features, y=target, cv=kfold)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "tn_filter = (predictions == 0) & (target == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "tp_filter = (predictions == 1) & (target == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "fn_filter = (predictions == 0) & (target == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "fp_filter = (predictions == 1) & (target == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "tpr = tp / (tp + fn)\n",
    "\n",
    "print(\"True Positive Rate;\", tpr)\n",
    "print(\"False Positive Rate:\", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, using a random forest classifier didn't improve our false positive rate. The model is likely weighting too heavily on the 1 class, and still mostly predicting 1s. We could fix this by applying a harsher penalty for misclassifications of 0s.\n",
    "\n",
    "Ultimately, our best model had a false positive rate of 19.6%, and a true positive rate of 20%. For a conservative investor, this means that they make money as long as the interest rate is high enough to offset the losses from 19.6% of borrowers defaulting, and that the pool of 20% of borrowers is large enough to make enough interest money to offset the losses. There is still a lot of improvement to be made on this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had randomly picked loans to fund, borrowers would have defaulted on 14.5% of them, and our model is better than that, although we're excluding more loans than a random strategy would. Given this, there's still quite a bit of room to improve:\n",
    "\n",
    "    We can tweak the penalties further.\n",
    "    We can try models other than a random forest and logistic regression.\n",
    "    We can use some of the columns we discarded to generate better features.\n",
    "    We can ensemble multiple models to get more accurate predictions.\n",
    "    We can tune the parameters of the algorithm to achieve higher performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
